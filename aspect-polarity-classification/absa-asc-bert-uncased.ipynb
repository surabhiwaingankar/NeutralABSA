{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers peft datasets torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5YBXLo6B1HL",
        "outputId": "b3f6f861-fa1c-4706-f4bb-0965abdada30"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "# from torch.utils.data import Dataset # Remove this line to avoid conflict\n",
        "from datasets import load_dataset, DatasetDict, Features, Value, Dataset # Import Dataset from datasets\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "YuCFTDzpB1im"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MAX_SEQ_LENGTH = 128\n",
        "SEED = 42\n",
        "set_seed(SEED)\n",
        "\n",
        "# Label encodings\n",
        "POLARITY_MAPPING = {\"Positive\": 0, \"Negative\": 1, \"Neutral\": 2}\n",
        "ASPECT_TAG_MAPPING = {\"B-ASP\": 0, \"I-ASP\": 1}\n",
        "\n",
        "# Helper function to handle errors\n",
        "def handle_error(error_msg, data_instance=None):\n",
        "    logging.error(error_msg)\n",
        "    if data_instance:\n",
        "        logging.error(f\"Data instance: {data_instance}\")\n"
      ],
      "metadata": {
        "id": "t8JV-rmiB4Jv"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataInstance:\n",
        "    sentence: str\n",
        "    aspects: List[dict] = field(default_factory=list)\n",
        "\n",
        "def read_atepc_data(file_path: str) -> List[DataInstance]:\n",
        "    \"\"\"Reads and parses the ATEPC data from a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = f.readlines()\n",
        "    except FileNotFoundError:\n",
        "        handle_error(f\"File not found: {file_path}\")\n",
        "        return []\n",
        "\n",
        "    data_instances = []\n",
        "    current_sentence = []\n",
        "    current_aspects = []\n",
        "    current_aspect_text = []\n",
        "    current_aspect_start = -1\n",
        "    current_aspect_polarity = None\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            if current_sentence:\n",
        "                sentence_str = \" \".join([word for word, _, _ in current_sentence])\n",
        "\n",
        "                # Add any remaining aspect\n",
        "                if current_aspect_text:\n",
        "                    current_aspects.append(\n",
        "                        {\n",
        "                            \"text\": \" \".join(current_aspect_text),\n",
        "                            \"start\": current_aspect_start,\n",
        "                            \"end\": current_aspect_start + len(current_aspect_text),\n",
        "                            \"polarity\": current_aspect_polarity,\n",
        "                            \"aspect_tag\": [ASPECT_TAG_MAPPING.get(tag) for _, tag, _ in current_sentence if tag != 'O' and tag != '-100'],\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                data_instances.append(\n",
        "                    DataInstance(sentence=sentence_str, aspects=current_aspects)\n",
        "                )\n",
        "                current_sentence = []\n",
        "                current_aspects = []\n",
        "                current_aspect_text = []\n",
        "                current_aspect_start = -1\n",
        "                current_aspect_polarity = None\n",
        "            continue\n",
        "\n",
        "        parts = line.split()\n",
        "        if len(parts) != 3:\n",
        "            handle_error(f\"Malformed line: {line}\", line)\n",
        "            continue\n",
        "\n",
        "        word, tag, polarity_str = parts\n",
        "        current_sentence.append((word, tag, polarity_str))\n",
        "\n",
        "        if tag == \"B-ASP\":\n",
        "            if current_aspect_text:\n",
        "                current_aspects.append(\n",
        "                    {\n",
        "                        \"text\": \" \".join(current_aspect_text),\n",
        "                        \"start\": current_aspect_start,\n",
        "                        \"end\": current_aspect_start + len(current_aspect_text),\n",
        "                        \"polarity\": current_aspect_polarity,\n",
        "                        \"aspect_tag\": [ASPECT_TAG_MAPPING.get(tag) for _, tag, _ in current_sentence if tag != 'O' and tag != '-100'],\n",
        "                    }\n",
        "                )\n",
        "            current_aspect_text = [word]\n",
        "            current_aspect_start = len(current_sentence) - 1\n",
        "            if polarity_str != '-100':\n",
        "                current_aspect_polarity = POLARITY_MAPPING.get(polarity_str)\n",
        "            else:\n",
        "                current_aspect_polarity = None\n",
        "\n",
        "        elif tag == \"I-ASP\":\n",
        "            if not current_aspect_text:\n",
        "                handle_error(\n",
        "                    f\"I-ASP tag without preceding B-ASP: {line}\", line\n",
        "                )\n",
        "            else:\n",
        "                current_aspect_text.append(word)\n",
        "                if polarity_str != '-100':\n",
        "                    current_aspect_polarity = POLARITY_MAPPING.get(polarity_str)\n",
        "\n",
        "        elif tag == \"O\":\n",
        "          if current_aspect_text:\n",
        "                current_aspects.append(\n",
        "                    {\n",
        "                        \"text\": \" \".join(current_aspect_text),\n",
        "                        \"start\": current_aspect_start,\n",
        "                        \"end\": current_aspect_start + len(current_aspect_text),\n",
        "                        \"polarity\": current_aspect_polarity,\n",
        "                        \"aspect_tag\": [ASPECT_TAG_MAPPING.get(tag) for _, tag, _ in current_sentence if tag != 'O' and tag != '-100'],\n",
        "                    }\n",
        "                )\n",
        "          current_aspect_text = []\n",
        "          current_aspect_start = -1\n",
        "          current_aspect_polarity = None\n",
        "\n",
        "    # Add the last sentence if it exists\n",
        "    if current_sentence:\n",
        "        sentence_str = \" \".join([word for word, _, _ in current_sentence])\n",
        "        if current_aspect_text:\n",
        "            current_aspects.append(\n",
        "                {\n",
        "                    \"text\": \" \".join(current_aspect_text),\n",
        "                    \"start\": current_aspect_start,\n",
        "                    \"end\": current_aspect_start + len(current_aspect_text),\n",
        "                    \"polarity\": current_aspect_polarity,\n",
        "                    \"aspect_tag\": [ASPECT_TAG_MAPPING.get(tag) for _, tag, _ in current_sentence if tag != 'O' and tag != '-100'],\n",
        "                }\n",
        "            )\n",
        "        data_instances.append(\n",
        "            DataInstance(sentence=sentence_str, aspects=current_aspects)\n",
        "        )\n",
        "\n",
        "    return data_instances"
      ],
      "metadata": {
        "id": "LEC0LFSIB6nb"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_and_create_dataset(data_instances: List[DataInstance]) -> DatasetDict:\n",
        "    \"\"\"Filters aspects with -100 polarity and creates a Hugging Face Dataset.\"\"\"\n",
        "    filtered_data = []\n",
        "    for instance in data_instances:\n",
        "        filtered_aspects = []\n",
        "        for aspect in instance.aspects:\n",
        "            if aspect[\"polarity\"] is not None:\n",
        "                filtered_aspects.append(aspect)\n",
        "            else:\n",
        "                logging.info(f\"Removed aspect due to -100 polarity: {aspect}\")\n",
        "        filtered_data.append(\n",
        "            {\"sentence\": instance.sentence, \"aspects\": filtered_aspects}\n",
        "        )\n",
        "\n",
        "    # Convert list of dictionaries to dictionary of lists\n",
        "    dataset_dict = {}\n",
        "    for key in filtered_data[0].keys():\n",
        "        dataset_dict[key] = [d[key] for d in filtered_data]\n",
        "\n",
        "    # Define the features explicitly\n",
        "    features = Features({\n",
        "        \"sentence\": Value(\"string\"),\n",
        "        \"aspects\": [\n",
        "            {\n",
        "                \"text\": Value(\"string\"),\n",
        "                \"start\": Value(\"int32\"),\n",
        "                \"end\": Value(\"int32\"),\n",
        "                \"polarity\": Value(\"int32\"),\n",
        "                \"aspect_tag\": [Value(\"int32\")]\n",
        "            }\n",
        "        ]\n",
        "    })\n",
        "\n",
        "    dataset = Dataset.from_dict(dataset_dict, features=features)\n",
        "\n",
        "    print(\"----- Dataset after filtering and creation -----\")\n",
        "    print(dataset)\n",
        "    print(dataset[0])\n",
        "\n",
        "    return DatasetDict({\"train\": dataset})\n"
      ],
      "metadata": {
        "id": "NK1XRVrsB_XM"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data (replace with your actual file paths)\n",
        "TRAIN_DATA_PATH = \"/content/train dataset.atepc\"\n",
        "TEST_DATA_PATH = \"/content/test dataset.atepc\"\n",
        "train_data_instances = read_atepc_data(TRAIN_DATA_PATH)\n",
        "test_data_instances = read_atepc_data(TEST_DATA_PATH)\n",
        "train_data = filter_and_create_dataset(train_data_instances)\n",
        "test_data = filter_and_create_dataset(test_data_instances)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H-IIcjSCE-Y",
        "outputId": "6d27f909-def0-4e25-c9a2-49604955eb6e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Dataset after filtering and creation -----\n",
            "Dataset({\n",
            "    features: ['sentence', 'aspects'],\n",
            "    num_rows: 31541\n",
            "})\n",
            "{'sentence': 'I charge it at night and skip taking the cord with me because of the good battery life .', 'aspects': [{'text': 'cord', 'start': 9, 'end': 10, 'polarity': 2, 'aspect_tag': [0]}]}\n",
            "----- Dataset after filtering and creation -----\n",
            "Dataset({\n",
            "    features: ['sentence', 'aspects'],\n",
            "    num_rows: 6060\n",
            "})\n",
            "{'sentence': 'Boot time is super fast , around anywhere from 35 seconds to 1 minute .', 'aspects': [{'text': 'Boot time', 'start': 0, 'end': 2, 'polarity': 0, 'aspect_tag': [0, 1]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYL6kd6lCYI_",
        "outputId": "81831500-f42a-49b6-b417-aade4cd862ee"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'aspects'],\n",
              "        num_rows: 31541\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2tIJmLkClJD",
        "outputId": "0e4f3a06-37db-462c-f5c2-25b2094fc0ba"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'aspects'],\n",
              "        num_rows: 6060\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5ut6e-XCmUa",
        "outputId": "b8699dc8-62b0-4f22-b085-9df8f002da94"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'aspects'],\n",
            "        num_rows: 31541\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSaJNs2OEceh",
        "outputId": "21e59784-3ed1-4a1d-deea-3abdf9f3d281"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['sentence', 'aspects'],\n",
            "        num_rows: 6060\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(min(5, len(train_data['train']))):\n",
        "    print(train_data['train'][i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQz7_HZNEgIx",
        "outputId": "3d2bb360-38c0-4ffd-f5f2-8151dc08cc4c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'sentence': 'I charge it at night and skip taking the cord with me because of the good battery life .', 'aspects': [{'text': 'cord', 'start': 9, 'end': 10, 'polarity': 2, 'aspect_tag': [0]}]}\n",
            "{'sentence': 'I charge it at night and skip taking the cord with me because of the good battery life .', 'aspects': [{'text': 'battery life', 'start': 16, 'end': 18, 'polarity': 0, 'aspect_tag': [0, 0, 1]}]}\n",
            "{'sentence': \"The tech guy then said the service center does not do 1 - to - 1 exchange and I have to direct my concern to the ` ` sales ' ' team , which is the retail shop which I bought my netbook from .\", 'aspects': [{'text': 'service center', 'start': 6, 'end': 8, 'polarity': 1, 'aspect_tag': [0, 1, 0, 1]}]}\n",
            "{'sentence': \"The tech guy then said the service center does not do 1 - to - 1 exchange and I have to direct my concern to the ` ` sales ' ' team , which is the retail shop which I bought my netbook from .\", 'aspects': [{'text': \"` ` sales '\", 'start': 26, 'end': 30, 'polarity': 1, 'aspect_tag': [0, 1, 0, 1, 0, 1, 1, 1]}]}\n",
            "{'sentence': \"The tech guy then said the service center does not do 1 - to - 1 exchange and I have to direct my concern to the ` ` sales ' ' team , which is the retail shop which I bought my netbook from .\", 'aspects': [{'text': 'tech guy', 'start': 1, 'end': 3, 'polarity': 2, 'aspect_tag': [0, 1]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "class ABSADataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_seq_length=128):\n",
        "        # super().__init__() # Call the parent class's __init__ is not needed anymore\n",
        "        self.dataset = dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        # Instead of assigning to self.data, create a new attribute\n",
        "        self.processed_data = self.preprocess_data()\n",
        "\n",
        "    def preprocess_data(self):\n",
        "        data = []\n",
        "        for item in self.dataset:\n",
        "            sentence = item['sentence']\n",
        "            aspects = item['aspects']\n",
        "\n",
        "            if isinstance(aspects, list) and aspects and isinstance(aspects[0], dict):\n",
        "                for aspect in aspects:\n",
        "                    aspect_text = aspect[\"text\"]\n",
        "                    aspect_start = sentence.find(aspect_text)\n",
        "                    aspect_end = aspect_start + len(aspect_text)\n",
        "\n",
        "                    modified_sentence = sentence[:aspect_start] + \"[ASP]\" + aspect_text + \"[ASP]\" + sentence[aspect_end:]\n",
        "\n",
        "                    encoding = self.tokenizer(\n",
        "                        modified_sentence,\n",
        "                        add_special_tokens=True,\n",
        "                        max_length=self.max_seq_length,\n",
        "                        padding=\"max_length\",\n",
        "                        truncation=True,\n",
        "                        return_tensors=\"pt\",\n",
        "                    )\n",
        "\n",
        "                    data.append({\n",
        "                \"input_ids\": encoding[\"input_ids\"].squeeze().tolist(),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].squeeze().tolist(),\n",
        "                \"labels\": int(aspect[\"polarity\"]),\n",
        "                # Add 'aspect_tag' to the data dictionary:\n",
        "                \"aspect_tag\": aspect[\"aspect_tag\"],  # Keep it as a list\n",
        "            })\n",
        "            else:\n",
        "                logging.warning(f\"Unexpected format for 'aspects' in item: {item}\")\n",
        "\n",
        "        # Convert the processed data into a Hugging Face Dataset\n",
        "        features = Features({\n",
        "            \"input_ids\": [Value(\"int64\")],  # Changed to list of int64\n",
        "            \"attention_mask\": [Value(\"int64\")],  # Changed to list of int64\n",
        "            \"labels\": Value(\"int64\"),\n",
        "            \"aspect_tag\": [Value(\"int32\")] # Define aspect_tag as a list of int32\n",
        "        })\n",
        "\n",
        "        # Using a dictionary comprehension to create the dataset input\n",
        "        # Ensures data is in correct format\n",
        "        dataset_input = {\n",
        "            k: [d[k] for d in data]\n",
        "            for k in [\"input_ids\", \"attention_mask\", \"labels\", \"aspect_tag\"]\n",
        "        }\n",
        "\n",
        "        return Dataset.from_dict(dataset_input, features=features)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)  # Use the processed_data attribute\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Modification: Handle list or int indices\n",
        "        if isinstance(idx, int):\n",
        "            return self.processed_data[idx]\n",
        "        elif isinstance(idx, list):\n",
        "            return [self.processed_data[i] for i in idx]\n",
        "        # Add support for slices\n",
        "        elif isinstance(idx, slice):\n",
        "            return self.processed_data[idx] # Return a slice of the data\n",
        "        else:\n",
        "            raise TypeError(\"Invalid index type. Expected int, list, or slice.\")\n",
        "\n",
        "    # Implement __getitems__ to handle batched indices\n",
        "    def __getitems__(self, keys):\n",
        "        \"\"\"\n",
        "        Handles batched indices by returning a list of dictionaries, each\n",
        "        representing a single data point.\n",
        "        \"\"\"\n",
        "        return [self[key] for key in keys]"
      ],
      "metadata": {
        "id": "KZUq9Kj9Er4k"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ABSADataCollator(DataCollatorWithPadding):\n",
        "    def __call__(self, features):\n",
        "        # Extract input features and labels\n",
        "        input_features = [{\"input_ids\": f[\"input_ids\"], \"attention_mask\": f[\"attention_mask\"]} for f in features]\n",
        "        labels = [f[\"labels\"] for f in features]\n",
        "\n",
        "        # Pad input features\n",
        "        batch = self.tokenizer.pad(\n",
        "            input_features,\n",
        "            padding=True,  # Use boolean padding\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=self.return_tensors,\n",
        "        )\n",
        "\n",
        "        # Add labels to the batch, converting each label to a tensor\n",
        "        batch[\"labels\"] = torch.tensor(labels)  # Convert to tensor directly\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "DClkPudsOje1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Recreate datasets\n",
        "train_dataset = ABSADataset(train_data['train'], tokenizer)\n",
        "test_dataset = ABSADataset(test_data['train'], tokenizer)"
      ],
      "metadata": {
        "id": "Fc_on0yfF5sf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----- Train Dataset -----\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLg-TCbxjOQy",
        "outputId": "74ad4545-b559-4945-a86a-e1ff4fc89dbd"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Train Dataset -----\n",
            "{'input_ids': [101, 1045, 3715, 2009, 2012, 2305, 1998, 13558, 2635, 1996, 1031, 2004, 2361, 1033, 11601, 1031, 2004, 2361, 1033, 2007, 2033, 2138, 1997, 1996, 2204, 6046, 2166, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2, 'aspect_tag': [0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n----- Test Dataset -----\")\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0qDHsljjPIC",
        "outputId": "6e23a6b3-5d5f-4606-b2c3-4d520988922e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Test Dataset -----\n",
            "{'input_ids': [101, 1031, 2004, 2361, 1033, 9573, 2051, 1031, 2004, 2361, 1033, 2003, 3565, 3435, 1010, 2105, 5973, 2013, 3486, 3823, 2000, 1015, 3371, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0, 'aspect_tag': [0, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update data collator\n",
        "data_collator = ABSADataCollator(tokenizer=tokenizer)  # Use the new data collator"
      ],
      "metadata": {
        "id": "_AlxPb4ejWSh"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, num_labels=3\n",
        ")  # 3 classes: Positive, Negative, Neutral\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYQzE3WuF8Sv",
        "outputId": "03a95121-4843-4127-d499-206b5936a203"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 297,219 || all params: 109,781,766 || trainable%: 0.2707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"absa_model\"\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=7,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    seed=SEED,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBNgiq2yGkrr",
        "outputId": "1bd8a356-f2ac-4f67-f6ea-a76a7233ff5d"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1_macro = f1_score(labels, predictions, average=\"macro\")\n",
        "    f1_weighted = f1_score(labels, predictions, average=\"weighted\")\n",
        "    conf_matrix = confusion_matrix(labels, predictions)\n",
        "\n",
        "    # Convert the confusion matrix to a list before returning\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1_macro,\n",
        "        \"f1_weighted\": f1_weighted,\n",
        "        \"confusion_matrix\": conf_matrix.tolist(),  # Convert to list\n",
        "    }"
      ],
      "metadata": {
        "id": "q8i5u6RpGmrK"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the original training dataset into train and validation sets\n",
        "train_val_dataset = train_dataset.dataset\n",
        "\n",
        "stratify_data = [len(item['aspects']) for item in train_val_dataset]\n",
        "\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(train_val_dataset)),\n",
        "    test_size=0.2,\n",
        "    random_state=SEED,\n",
        "    stratify=stratify_data\n",
        ")\n",
        "\n",
        "train_hf_dataset = train_val_dataset.select(train_idx.tolist())\n",
        "val_hf_dataset = train_val_dataset.select(val_idx.tolist())\n",
        "\n",
        "train_dataset = ABSADataset(train_hf_dataset, tokenizer)\n",
        "val_dataset = ABSADataset(val_hf_dataset, tokenizer)\n"
      ],
      "metadata": {
        "id": "Ofz-0xNNIEb3"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ipython-input-71-fdda2b3d9e86\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset.processed_data,  # Use the processed Hugging Face Dataset\n",
        "    eval_dataset=val_dataset.processed_data,  # Use the processed Hugging Face Dataset\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "iRc-SWfjGodq",
        "outputId": "a289ee8b-4f7e-4d4c-8637-a7e4702f1ccf"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8834' max='8834' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8834/8834 41:33, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>Confusion Matrix</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.794100</td>\n",
              "      <td>0.755579</td>\n",
              "      <td>0.645602</td>\n",
              "      <td>0.598061</td>\n",
              "      <td>0.618110</td>\n",
              "      <td>[[1858, 138, 198], [172, 991, 168], [608, 505, 410]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.732200</td>\n",
              "      <td>0.707557</td>\n",
              "      <td>0.674723</td>\n",
              "      <td>0.643112</td>\n",
              "      <td>0.661651</td>\n",
              "      <td>[[1885, 96, 213], [160, 906, 265], [533, 375, 615]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.736300</td>\n",
              "      <td>0.665203</td>\n",
              "      <td>0.706616</td>\n",
              "      <td>0.684235</td>\n",
              "      <td>0.699187</td>\n",
              "      <td>[[1865, 101, 228], [130, 958, 243], [456, 323, 744]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.641800</td>\n",
              "      <td>0.641087</td>\n",
              "      <td>0.725040</td>\n",
              "      <td>0.710016</td>\n",
              "      <td>0.722418</td>\n",
              "      <td>[[1803, 112, 279], [105, 997, 229], [360, 303, 860]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.670200</td>\n",
              "      <td>0.632941</td>\n",
              "      <td>0.729596</td>\n",
              "      <td>0.714268</td>\n",
              "      <td>0.726647</td>\n",
              "      <td>[[1838, 102, 254], [120, 966, 245], [382, 262, 879]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.635100</td>\n",
              "      <td>0.624991</td>\n",
              "      <td>0.732964</td>\n",
              "      <td>0.719526</td>\n",
              "      <td>0.731176</td>\n",
              "      <td>[[1811, 105, 278], [108, 984, 239], [353, 265, 905]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.653100</td>\n",
              "      <td>0.625800</td>\n",
              "      <td>0.735341</td>\n",
              "      <td>0.720610</td>\n",
              "      <td>0.732569</td>\n",
              "      <td>[[1836, 104, 254], [111, 989, 231], [367, 269, 887]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8834, training_loss=0.7126210201696112, metrics={'train_runtime': 2494.0632, 'train_samples_per_second': 56.669, 'train_steps_per_second': 3.542, 'total_flos': 9329177456460288.0, 'train_loss': 0.7126210201696112, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_results = trainer.evaluate(test_dataset.processed_data)\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "WShBqO8lKQz-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "e63166e4-80e7-4ebe-fc88-b55352110a1b"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [190/190 00:47]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5667867064476013, 'eval_accuracy': 0.7657152285101468, 'eval_f1': 0.7259628038250888, 'eval_f1_weighted': 0.7622398213584392, 'eval_confusion_matrix': [[2748, 167, 262], [130, 1109, 198], [387, 276, 784]], 'eval_runtime': 48.2499, 'eval_samples_per_second': 125.617, 'eval_steps_per_second': 3.938, 'epoch': 7.0}\n"
          ]
        }
      ]
    }
  ]
}